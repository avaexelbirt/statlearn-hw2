---
title: "SLHW2"
author: "Ava Exelbirt, Sam Reade"
date: "2024-12-01"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Install necessary libraries if not installed
#install.packages(c("ggplot2", "dplyr", "scales", "lubridate"))
# install.packages("caret")
# install.packages("GGally")
# install.packages("tidyverse")
#install.packages("randomForest")
# install.packages("gbm")
# install.packages("fastshap")
#install.packages("effects")
#install.packages("ROCR")
```

```{r libraries}
# Load libraries
library(ggplot2)
library(dplyr)
library(scales)
library(lubridate)
library(tidyverse)
library(GGally)
library(caret)
library(rpart)
library(rpart.plot)
library(nnet)
library(gbm)
library(MASS)
library(randomForest)
library(pdp)
library(fastshap)
library(effects)
library(e1071)
library(ROCR)
```

# About the Data

### Import Data

```{r data}
#tuesdata <- tidytuesdayR::tt_load('2022-11-01')
tuesdata <- tidytuesdayR::tt_load(2022, week = 44)
horror <- tuesdata$horror_movies
```

```{r glimpse}
glimpse(horror)
```

### Data Dictionary

1.  The `id` variable is an integer that serves as a unique identifier for each movie.
2.  The `original_title` variable is a character string representing the movie's original title.
3.  The `title` variable is a character string containing the localized or alternative movie title.
4.  The `original_language` variable is a character field indicating the language in which the movie was originally made.
5.  The `overview` variable is a character field providing a brief description or synopsis of the movie.
6.  The `tagline` variable is a character field capturing the movie's catchphrase or slogan.
7.  The `release_date` variable is a date field that records the date when the movie was first released.
8.  The `poster_path` variable is a character field containing the URL to the movie's poster image.
9.  The `popularity` variable is a numerical value representing the movie's popularity score based on audience interactions.
10. The `vote_count` variable is an integer field that records the total number of audience votes received.
11. The `vote_average` variable is a numerical field that represents the average audience rating on a scale from 0 to 10.
12. The `budget` variable is an integer field capturing the movie's production budget in USD.
13. The `revenue` variable is an integer field indicating the total revenue earned by the movie in USD.
14. The `runtime` variable is an integer field that specifies the duration of the movie in minutes.
15. The `status` variable is a character field that indicates the current status of the movie, such as "Released."
16. The `adult` variable is a boolean that indicates whether the movie is intended for adult audiences.
17. The `backdrop_path` variable is a character field that provides the URL to the backdrop image for the movie.
18. The `genre_names` variable is a character field listing the genres associated with the movie, separated by commas.
19. The `collection` variable is a numerical field containing the unique ID of the collection the movie belongs to, which may be null for movies not part of a collection.
20. The `collection_name` variable is a character field representing the name of the collection, which may also be null if the movie does not belong to one.

### Available Data

The dataset contains detailed information on a wide range of horror movies, about \~35,000 pieces of entertainment, including various features such as title, genre, release date, runtime, popularity, budget, and revenue. Additional details include the movie's runtime, vote count, average vote, genre names, and collection association. Notably, the dataset also contains the poster and backdrop image URLs for each movie, as well as whether the movie is intended for adults. These data points provide a comprehensive view of each movie's performance, reception, and thematic elements, enabling further analysis on trends, movie popularity, and financial success within the horror genre. These features will be used to train a classification model to predict whether each entry is a successful movie or not. The objective is to leverage these data points to build an accurate classification model, focusing on identifying the key predictors that contribute most to the classification process.

### Motivation

As the entertainment industry expands, identifying the success of a movie is critical to content platforms and production companies. Success in the entertainment industry is typically measured by revenue, budget, and audience reception. Predicting whether a movie is likely to be successful or not can help guide investment decisions, optimize content strategies, and improve user recommendations. However, accurately predicting success is a challenge due to the multifaceted nature of what contributes to a movie's success, including budget, genre, release time, and audience engagement factors.

In this context, predicting a movie's success involves analyzing historical data and identifying patterns that correlate with positive outcomes. By doing so, production teams and platforms can better allocate resources, strategize marketing efforts, and predict the potential success of future movies. The motivation behind this project is to build a classification model that can predict whether a movie will be successful based on various features, thus improving decision-making processes in the entertainment industry.

### Goal

The primary goal of this project is to develop a classification model that predicts whether a given movie is successful or not. The project will focus on feature selection, model interpretation, and the comparison of predictor sets to determine the most significant factors contributing to a movie's success. By analyzing a variety of features such as budget, revenue, genre, and popularity, the goal is to build a model that classifies movies as "successful" or "unsuccessful" with high accuracy. This will allow content platforms and production companies to make data-driven decisions and better understand the elements that contribute to the success of a movie.

# Data Preprocessing and Visualization Tools

```{r summ}
summary(horror)
```

## Data Cleanup

### Handling NA Values

We will look at how many NA values are in each column to better understand our data set.

```{r NA-count}
na_counts <- colSums(is.na(horror))

print(na_counts)
```

```{r}
sum(horror$revenue == 0)
sum(horror$budget == 0)
sum(horror$budget != 0 & horror$revenue != 0)
sum(horror$budget == 0 & horror$revenue == 0)

```

For numeric columns, we will fill missing values with the median values of that column. These include id, release_date, popularity, vote_count, vote_average, revenue, and runtime. We will then fill missing character columns with "Unknown." These include original_title, title, original_language, tagline, overview, poster_path, status, adult, and backdrop_path.

```{r NAs}
numeric_cols <- sapply(horror, is.numeric)
horror[numeric_cols] <- lapply(horror[numeric_cols], function(x) {
  ifelse(is.na(x), median(x, na.rm = TRUE), x)
})

char_cols <- sapply(horror, is.character)
horror[char_cols] <- lapply(horror[char_cols], function(x) {
  ifelse(is.na(x), "Unknown", x)
})

```

### Drop Columns

We will remove the columns ids and paths as these are not needed for our overall analysis.

```{r}
#horror <- horror |> select(-id, -poster_path, -backdrop_path, -collection, -collection_name)

horror <- dplyr::select(horror, -id, -poster_path, -backdrop_path, -collection, -collection_name)


```

### Feature Engineering

As part of feature engineering we need to create our boolean-like columns to logical data types. We will do so for the adult column. If the observation is FALSE, then it will convert to a logical operator of 0. If the observation is TRUE for this column, then it will be converted to 1. We must also convert categorical columns to factors. This includes original_language, status, and genre_names. Finally, we will extract year from release_date because this will help in further analysis.

```{r feature-eng}
horror$adult <- as.logical(horror$adult)

categorical_cols <- c("original_language", "status", "genre_names")
horror[categorical_cols] <- lapply(horror[categorical_cols], as.factor)

horror$release_year <- as.numeric(substr(horror$release_date, 1, 4))
```

### Handling Outliers

We will replace some outliers. Specifically, for runtime we will replace runtime with the 0 if there is a runtime that is defined as an outlier, we will replace it with 0. We will also remove rows with outliers regarding popularity that is defined as popularity above 10000. We will also categorize budget levels. We categorize movies into "Low", "Medium", or "High" budget based on the budget column:

```{r outliers}
runQ1 <- quantile(horror$runtime, 0.25, na.rm = TRUE)
runQ3 <- quantile(horror$runtime, 0.75, na.rm = TRUE)
IQR <- runQ3 - runQ1
lower_bound <- runQ1 - 1.5 * IQR
upper_bound <- runQ3 + 1.5 * IQR
horror$runtime[horror$runtime < lower_bound | horror$runtime > upper_bound] <- 0

#horror$runtime[horror$runtime <= 0 | horror$runtime > 300] <- NA

horror <- horror[!(horror$popularity > 10000), ]

horror$budget_category <- ifelse(horror$budget == 0, "No Budget",
                           ifelse(horror$budget < 1e7, "Low",
                           ifelse(horror$budget < 5e7, "Medium", "High")))
```

### Create Target Variables

We first create a profit variable that is the revenue minus budget of a movie. We then create a success variable: if profit \> 0, movie is considered successful

```{r make-targ}
horror$profit <- horror$revenue - horror$budget
horror$success <- ifelse(horror$profit > 0, "Success", "No Success")
```

### Making data frame with no 0s

Create a new data frame without rows where revenue and budget is 0

```{r}
df_for_class = subset(horror, revenue != 0 & budget != 0)
df_for_class$success = as.factor(df_for_class$success)

head(df_for_class)
dim(df_for_class)
```

## Split the Data

### Train and Test Data

We will split the data into training (60%) and testing (40%) sets. We will then look at the new data by checking the number of rows in training and testing sets and looking at the summary of the training set.

```{r split}
set.seed(123)

in_train <- createDataPartition(df_for_class$budget_category, p = 0.6, list = FALSE)
training <- df_for_class[in_train, ]
testing <- df_for_class[-in_train, ]

nrow(training)
nrow(testing)

summary(training)
```

### Distribution of Target Variable

```{r show-dist}
table(training$success) / length(training$success)
```

### Correlation Analysis

```{r corr}
ggcorr(df_for_class[ , sapply(df_for_class, is.numeric)], label = TRUE)
```

## Visualization Tools

### Distribution of Numeric Features

We will plot the distributions of numeric features, specifically budget, revenue, and runtime.

```{r}
ggplot(df_for_class, aes(x = budget)) +
  geom_histogram(binwidth = 1e7, fill = "blue", color = "black", alpha = 0.7) + 
  labs(title = "Distribution of Budget", x = "Budget", y = "Count") +
  theme_minimal()

ggplot(df_for_class, aes(x = revenue)) +
  geom_histogram(binwidth = 1e8, fill = "green", color = "black", alpha = 0.7) +  
  labs(title = "Distribution of Revenue", x = "Revenue", y = "Count") +
  theme_minimal()

ggplot(df_for_class, aes(x = runtime)) +
  geom_histogram(binwidth = 5, fill = "red", color = "black", alpha = 0.7) +  
  labs(title = "Distribution of Runtime", x = "Runtime (Minutes)", y = "Count") +
  theme_minimal()

```

Budget and revenue are both right skewed and unimodal. Runtime is also right skewed, but could be considered to be bimodal.

### Distribution of Some Categorical Features

We will plot the distributions of some categorical features, specifically release_year and budget_category.

```{r cat-dist}
ggplot(df_for_class, aes(x = release_year)) +
  geom_bar(fill = "purple", color = "black") +
  labs(title = "Distribution of Movies by Release Year", x = "Release Year", y = "Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

ggplot(df_for_class, aes(x = budget_category, fill = budget_category)) +
  geom_bar() +
  labs(title = "Distribution of Movies by Budget Category", x = "Budget Category", y = "Count") +
  theme_minimal() +
  theme(legend.position = "none")

```

Release year is left skewed and unimodal. Most movies were made with a low budget and only very few movies were made with a high budget.

### Distribution of Target Variable

We will now look at the distribution of our target variable, show_or_movie.

```{r}
sum(df_for_class$success == "Success")
sum(df_for_class$success == "No Success")
```

66.7% successful movies, 33.3% unsuccessful movies: unbalanced dataset

```{r horrordist}
ggplot(df_for_class, aes(x = success)) + 
  geom_bar(fill = "lightblue", color = "black") + 
  labs(title = "Distribution of Success of a Movie", x = "Success", y = "Count") + 
  theme_minimal()
```

### Variable Relationships

We first examine the relationship between budget and revenue for each horror movie, with points colored by their budget category, helping to identify patterns and outliers in how budget impacts revenue. We can also visualize how the budget has evolved over the years by plotting release_year versus budget. We then examine the relationship between `popularity` and `vote_average` to see if there's a trend in how movies' popularity correlates with their ratings. We also examine the relationship between `budget` and `profit`. Each point represents a movie, and the color differentiates between successful and non-successful movies. The idea is to see if movies with higher budgets tend to generate more profit. Finally we show how the `profit` distribution differs between successful and non-successful movies. This boxplot shows the distribution of `profit` for movies categorized as "Success" or "No Success". The plot helps visualize how profits are distributed across these categories, revealing if successful movies tend to have higher profits.

```{r}
ggplot(df_for_class, aes(x = budget, y = revenue)) +
  geom_point(aes(color = budget_category), alpha = 0.7) +
  scale_x_continuous(labels = scales::comma) +
  scale_y_continuous(labels = scales::comma) +
  labs(title = "Budget vs Revenue", x = "Budget", y = "Revenue") +
  theme_minimal() +
  theme(legend.title = element_blank())

ggplot(df_for_class, aes(x = release_year, y = budget)) +
  geom_point(aes(color = budget_category), alpha = 0.7) +
  scale_x_continuous(labels = scales::comma) +
  scale_y_continuous(labels = scales::comma) +
  labs(title = "Release Year vs Budget")

ggplot(df_for_class, aes(x = popularity, y = vote_average)) +
  geom_point(aes(color = genre_names), alpha = 0.7) +
  scale_x_continuous(labels = scales::comma) +
  labs(title = "Popularity vs Vote Average", x = "Popularity", y = "Vote Average") +
  theme_minimal() +
  theme(legend.position = "none")

ggplot(df_for_class, aes(x = budget, y = profit)) + 
  geom_point(aes(color = success), alpha = 0.6) + 
  scale_x_continuous(labels = scales::comma) + 
  scale_y_continuous(labels = scales::comma) + 
  labs(title = "Budget vs. Profit", x = "Budget", y = "Profit") + 
  theme_minimal() + 
  theme(legend.title = element_blank())

ggplot(df_for_class, aes(x = success, y = profit, fill = success)) + 
  geom_boxplot() + 
  scale_y_continuous(labels = scales::comma) + 
  labs(title = "Profit Distribution by Success", x = "Success", y = "Profit") + 
  theme_minimal() + 
  theme(legend.title = element_blank())

```

### Training Visuals

We will now do EDA but for data in the training data set instead of the original data set to see how the ditrubutions change or not for each variable.

```{r}
ggplot(training, aes(x = budget)) + 
  geom_density(fill = "lightblue") + 
  labs(title = "Density of Budget", x = "Budget", y = "Density")

ggplot(training, aes(x = revenue)) + 
  geom_density(fill = "lightgreen") + 
  labs(title = "Density of Revenue", x = "Revenue", y = "Density")

ggplot(training, aes(fill = budget_category, x = revenue)) + 
  geom_density() + 
  labs(title = "Density of Revenue and Budget", x = "Revenue", y = "Density")

ggplot(training, aes(x = runtime)) + 
  geom_density(fill = "lightcoral") + 
  labs(title = "Density of Runtime", x = "Runtime", y = "Density")

ggplot(training, aes(x = popularity)) + 
  geom_density(fill = "lightyellow") + 
  labs(title = "Density of Popularity", x = "Popularity", y = "Density")

ggplot(training, aes(x = runtime, fill = budget_category)) + 
  geom_density(alpha = 0.5) + 
  labs(title = "Runtime by Budget Category", x = "Runtime", y = "Density")

ggplot(training, aes(x = vote_average)) + 
  geom_density(fill = "lightblue") + 
  labs(title = "Density of Vote Average", x = "Vote Average", y = "Density")

ggplot(training, aes(x = release_year)) + 
  geom_bar(fill = "lightgreen") + 
  labs(title = "Release Year Distribution", x = "Release Year", y = "Count")

ggplot(training, aes(x = success)) + 
  geom_bar(fill = "lightblue", color = "black") + 
  labs(title = "Distribution of Success in Training Set", x = "Success", y = "Count") + 
  theme_minimal()

```

# Classification with Emphasis on Prediction

## Questions We Aim to Answer:

1.  "How well can we predict a movie's revenue based on its budget, popularity, and release year?" (Regression)
2.  Predicting popularity (Regression)
3.  "Can we predict whether a movie will be profitable?" (QDA and LDA)
4.  "Predicting success categories (hit, average, flop) (QDA and LDA)
5.  Predicting Budget using logistic regression
6.  Predicting whether a movie was a success or not is our main objective.

## Normalizing the Data

We will normalize the data with the min-max normalization function which will later be split into training and testing and used for models that need normalized data.

```{r}
# Columns to normalize
columns_to_normalize <- c("budget", "release_year", "runtime", "popularity")
# Min-max normalization function
min_max_normalize <- function(x) {
  (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))
}
# Create a copy of the original data frame
df_normal <- df_for_class
# Normalize only the specified columns
df_normal[columns_to_normalize] <- lapply(df_for_class[columns_to_normalize], min_max_normalize)
# Check the structure to confirm normalization
str(df_normal)

```

## Splitting training and testing for normalized data

```{r}
in_train_n <- createDataPartition(df_normal$budget_category, p = 0.6, list = FALSE)
training_n <- df_normal[in_train, ]
testing_n <- df_normal[-in_train, ]

nrow(training_n)
nrow(testing_n)

summary(training_n)
```

## Visualizing the Relationships

We will visualize the relationships between the variables we chose to use to revenue, colored by success.

```{r}
library(ggplot2)

ggplot(df_for_class, aes(x = budget, y = revenue, color = success)) +
  geom_point() + 
  labs(title = "Budget vs Revenue and Success", x = "Budget", y = "Revenue") +
  theme_minimal()

ggplot(df_for_class, aes(x = release_year, y = revenue, color = success)) +
  geom_point() + 
  labs(title = "Release Year vs Revenue and Success", x = "Release Year", y = "Revenue") +
  theme_minimal()

ggplot(df_for_class, aes(x = popularity, y = revenue, color = success)) +
  geom_point() + 
  labs(title = "Popularity vs Revenue and Success", x = "Popularity", y = "Revenue") +
  theme_minimal()

ggplot(df_for_class, aes(x = runtime, y = revenue, color = success)) +
  geom_point() + 
  labs(title = "Runtime vs Revenue and Success", x = "Runtime", y = "Revenue") +
  theme_minimal()

```

## Classifying Success with LDA:

### Training the model

```{r}
lda_model <- lda(success ~ budget + release_year + runtime + popularity, data = training_n)
lda_model
```

### Predicting

We will first predict success categories and then add the predictions to the LDA dataset for visualizing the predictions.

```{r}
lda_predictions <- predict(lda_model, newdata = testing_n)$class

prediction_counts_lda <- table(lda_predictions)
print(prediction_counts_lda)

```

### Evaluating the Model

We will use the confusion matrix and accuracy calculations to show the accuracy of the model.

```{r}
confusion_matrix_lda <- table(Predicted = lda_predictions, Actual = testing_n$success)
print(confusion_matrix_lda)

accuracy_lda <- sum(lda_predictions == testing_n$success) / length(lda_predictions)
print(paste("Accuracy: ", round(accuracy_lda * 100, 4), "%"))

```

### Visualizing Decision Boundaries

```{r}
testing_n$lda_predicted_success <- lda_predictions

library(ggplot2)
ggplot(testing_n, aes(x = budget, y = popularity, color = lda_predicted_success)) +
  geom_point(alpha = 0.6) +
  labs(title = "LDA Predictions: Success vs. No Success", x = "Budget", y = "Popularity") +
  theme_minimal()

```

### PDP

```{r}

budget_range <- seq(from = min(training_n$budget, na.rm = TRUE), 
                    to = max(training_n$budget, na.rm = TRUE), 
                    length.out = 100)

pdp_data <- data.frame(
  budget = budget_range,
  release_year = rep(mean(training_n$release_year, na.rm = TRUE), 100),
  runtime = rep(mean(training_n$runtime, na.rm = TRUE), 100),
  popularity = rep(mean(training_n$popularity, na.rm = TRUE), 100)
)

pred_probs <- predict(lda_model, newdata = pdp_data, type = "response")$posterior[,2]

pdp_data$success_probability <- pred_probs

ggplot(pdp_data, aes(x = budget, y = success_probability)) +
  geom_line() +
  labs(title = "Partial Dependence of Success on Budget (LDA)",
       x = "Budget",
       y = "Probability of Success") +
  theme_minimal()




pop_range <- seq(from = min(training_n$popularity, na.rm = TRUE), 
                    to = max(training_n$popularity, na.rm = TRUE), 
                    length.out = 100)

pdp_data2 <- data.frame(
  popularity = pop_range,
  release_year = rep(mean(training_n$release_year, na.rm = TRUE), 100),
  runtime = rep(mean(training_n$runtime, na.rm = TRUE), 100),
  budget = rep(mean(training_n$budget, na.rm = TRUE), 100)
)

pred_probs2 <- predict(lda_model, newdata = pdp_data2, type = "response")$posterior[,2]

pdp_data2$success_probability <- pred_probs2

ggplot(pdp_data2, aes(x = popularity, y = success_probability)) +
  geom_line() +
  labs(title = "Partial Dependence of Success on popularity (LDA)",
       x = "popularity",
       y = "Probability of Success") +
  theme_minimal()


rt_range <- seq(from = min(training_n$runtime, na.rm = TRUE), 
                    to = max(training_n$runtime, na.rm = TRUE), 
                    length.out = 100)

pdp_data3 <- data.frame(
  runtime = rt_range,
  release_year = rep(mean(training_n$release_year, na.rm = TRUE), 100),
  popularity = rep(mean(training_n$popularity, na.rm = TRUE), 100),
  budget = rep(mean(training_n$budget, na.rm = TRUE), 100)
)

pred_probs3 <- predict(lda_model, newdata = pdp_data3, type = "response")$posterior[,2]

pdp_data3$success_probability <- pred_probs3

ggplot(pdp_data3, aes(x = runtime, y = success_probability)) +
  geom_line() +
  labs(title = "Partial Dependence of Success on runtime (LDA)",
       x = "runtime",
       y = "Probability of Success") +
  theme_minimal()


ry_range <- seq(from = min(training_n$release_year, na.rm = TRUE), 
                    to = max(training_n$release_year, na.rm = TRUE), 
                    length.out = 100)

pdp_data4 <- data.frame(
  release_year = ry_range,
  runtime = rep(mean(training_n$runtime, na.rm = TRUE), 100),
  popularity = rep(mean(training_n$popularity, na.rm = TRUE), 100),
  budget = rep(mean(training_n$budget, na.rm = TRUE), 100)
)

pred_probs4 <- predict(lda_model, newdata = pdp_data4, type = "response")$posterior[,2]

pdp_data4$success_probability <- pred_probs4

ggplot(pdp_data4, aes(x = release_year, y = success_probability)) +
  geom_line() +
  labs(title = "Partial Dependence of Success on release year (LDA)",
       x = "release year",
       y = "Probability of Success") +
  theme_minimal()




```

## Classifying Success with QDA

### Training the model

```{r}
qda_model = qda(success ~ budget + release_year + runtime + popularity, data = training_n)

qda_model
```

### Predicting

```{r}
qda_predictions = predict(qda_model, newdata = testing_n)$class

prediction_counts_qda <- table(qda_predictions)
print(prediction_counts_qda)

```

### Evaluating the Model

We will use the confusion matrix and accuracy calculations to show the accuracy of the model.

```{r}
confusion_matrix_qda <- table(Predicted = qda_predictions, Actual = testing_n$success)
print(confusion_matrix_qda)

accuracy_qda <- sum(qda_predictions == testing_n$success) / length(qda_predictions)
print(paste("Accuracy: ", round(accuracy_qda * 100, 4), "%"))
```

### PDP

```{r}
budget_range_qda <- seq(from = min(training_n$budget, na.rm = TRUE), 
                    to = max(training_n$budget, na.rm = TRUE), 
                    length.out = 100)

pdp_data_qda <- data.frame(
  budget = budget_range_qda,
  release_year = rep(mean(training_n$release_year, na.rm = TRUE), 100),
  runtime = rep(mean(training_n$runtime, na.rm = TRUE), 100),
  popularity = rep(mean(training_n$popularity, na.rm = TRUE), 100)
)

pred_probs_qda <- predict(qda_model, newdata = pdp_data_qda, type = "response")$posterior[,2]

pdp_data_qda$success_probability <- pred_probs_qda

ggplot(pdp_data_qda, aes(x = budget, y = success_probability)) +
  geom_line() +
  labs(title = "Partial Dependence of Success on Budget (qda)",
       x = "Budget",
       y = "Probability of Success") +
  theme_minimal()


pop_range_qda <- seq(from = min(training_n$popularity, na.rm = TRUE), 
                    to = max(training_n$popularity, na.rm = TRUE), 
                    length.out = 100)

pdp_data2_qda <- data.frame(
  popularity = pop_range_qda,
  release_year = rep(mean(training_n$release_year, na.rm = TRUE), 100),
  runtime = rep(mean(training_n$runtime, na.rm = TRUE), 100),
  budget = rep(mean(training_n$budget, na.rm = TRUE), 100)
)

pred_probs2_qda <- predict(qda_model, newdata = pdp_data2_qda, type = "response")$posterior[,2]

pdp_data2_qda$success_probability <- pred_probs2_qda

ggplot(pdp_data2_qda, aes(x = popularity, y = success_probability)) +
  geom_line() +
  labs(title = "Partial Dependence of Success on popularity (qda)",
       x = "popularity",
       y = "Probability of Success") +
  theme_minimal()


rt_range_qda <- seq(from = min(training_n$runtime, na.rm = TRUE), 
                    to = max(training_n$runtime, na.rm = TRUE), 
                    length.out = 100)

pdp_data3_qda <- data.frame(
  runtime = rt_range_qda,
  release_year = rep(mean(training_n$release_year, na.rm = TRUE), 100),
  popularity = rep(mean(training_n$popularity, na.rm = TRUE), 100),
  budget = rep(mean(training_n$budget, na.rm = TRUE), 100)
)

pred_probs3_qda <- predict(qda_model, newdata = pdp_data3_qda, type = "response")$posterior[,2]

pdp_data3_qda$success_probability <- pred_probs3_qda

ggplot(pdp_data3_qda, aes(x = runtime, y = success_probability)) +
  geom_line() +
  labs(title = "Partial Dependence of Success on runtime (qda)",
       x = "runtime",
       y = "Probability of Success") +
  theme_minimal()


ry_range_qda <- seq(from = min(training_n$release_year, na.rm = TRUE), 
                    to = max(training_n$release_year, na.rm = TRUE), 
                    length.out = 100)

pdp_data4_qda <- data.frame(
  release_year = ry_range_qda,
  runtime = rep(mean(training_n$runtime, na.rm = TRUE), 100),
  popularity = rep(mean(training_n$popularity, na.rm = TRUE), 100),
  budget = rep(mean(training_n$budget, na.rm = TRUE), 100)
)

pred_probs4_qda <- predict(qda_model, newdata = pdp_data4, type = "response")$posterior[,2]

pdp_data4_qda$success_probability <- pred_probs4_qda

ggplot(pdp_data4_qda, aes(x = release_year, y = success_probability)) +
  geom_line() +
  labs(title = "Partial Dependence of Success on release year (qda)",
       x = "release year",
       y = "Probability of Success") +
  theme_minimal()
```

## Classifying Success with Naive Bayes:

Naive Bayes is a probabilistic model based on Bayes' Theorem. The model assumes that the features are conditionally independent given the target variable, success.

### Training the Model

```{r}
nb_model <- naiveBayes(success ~ budget + release_year + runtime + popularity, data = training)

print(nb_model)

```

### Predicting

```{r}
predictions_nb <- predict(nb_model, testing)

predictions_nb_counts <- table(predictions_nb)
print(predictions_nb_counts)

```

### Evaluating the Model

We will use a confusion matrix to show model accuracy.

```{r}
confusion_matrix_nb <- table(Predicted_NB = predictions_nb, Actual = testing$success)
print(confusion_matrix_nb)


accuracy_nb <- sum(predictions_nb == testing$success) / nrow(testing)
print(paste("Accuracy: ", round(accuracy_nb *100, 4), "%"))


```

### PDP

```{r}
budget_range_nb <- seq(from = min(training_n$budget, na.rm = TRUE), 
                    to = max(training_n$budget, na.rm = TRUE), 
                    length.out = 100)

pdp_data_nb <- data.frame(
  budget = budget_range_nb,
  release_year = rep(mean(training_n$release_year, na.rm = TRUE), 100),
  runtime = rep(mean(training_n$runtime, na.rm = TRUE), 100),
  popularity = rep(mean(training_n$popularity, na.rm = TRUE), 100)
)

pred_probs_nb <- predict(nb_model, newdata = pdp_data_nb, type = "raw")

success_probability <- pred_probs_nb[, 2]

pdp_data_nb$success_probability <- success_probability

ggplot(pdp_data_nb, aes(x = budget, y = success_probability)) +
  geom_line() +
  labs(title = "Partial Dependence of Success on Budget (nb)",
       x = "Budget",
       y = "Probability of Success") +
  theme_minimal()



pop_range_nb <- seq(from = min(training_n$popularity, na.rm = TRUE), 
                    to = max(training_n$popularity, na.rm = TRUE), 
                    length.out = 100)

pdp_data2_nb <- data.frame(
  popularity = pop_range_nb,
  release_year = rep(mean(training_n$release_year, na.rm = TRUE), 100),
  runtime = rep(mean(training_n$runtime, na.rm = TRUE), 100),
  budget = rep(mean(training_n$budget, na.rm = TRUE), 100)
)

pred_probs_nb2 <- predict(nb_model, newdata = pdp_data2_nb, type = "raw")

success_probability2 <- pred_probs_nb[, 2]

pdp_data2_nb$success_probability2 <- success_probability2

ggplot(pdp_data2_nb, aes(x = popularity, y = success_probability2)) +
  geom_line() +
  labs(title = "Partial Dependence of Success on popularity (nb)",
       x = "popularity",
       y = "Probability of Success") +
  theme_minimal()


rt_range_nb <- seq(from = min(training_n$runtime, na.rm = TRUE), 
                    to = max(training_n$runtime, na.rm = TRUE), 
                    length.out = 100)

pdp_data3_nb <- data.frame(
  runtime = rt_range_nb,
  release_year = rep(mean(training_n$release_year, na.rm = TRUE), 100),
  popularity = rep(mean(training_n$popularity, na.rm = TRUE), 100),
  budget = rep(mean(training_n$budget, na.rm = TRUE), 100)
)

pred_probs3_nb <- predict(nb_model, newdata = pdp_data3_nb, type = "raw")

success_probability3 <- pred_probs3_nb[, 2]

pdp_data3_nb$success_probability3 <- success_probability3

ggplot(pdp_data3_nb, aes(x = runtime, y = success_probability3)) +
  geom_line() +
  labs(title = "Partial Dependence of Success on runtime (nb)",
       x = "runtime",
       y = "Probability of Success") +
  theme_minimal()


ry_range_nb <- seq(from = min(training_n$release_year, na.rm = TRUE), 
                    to = max(training_n$release_year, na.rm = TRUE), 
                    length.out = 100)

pdp_data4_nb <- data.frame(
  release_year = ry_range_nb,
  runtime = rep(mean(training_n$runtime, na.rm = TRUE), 100),
  popularity = rep(mean(training_n$popularity, na.rm = TRUE), 100),
  budget = rep(mean(training_n$budget, na.rm = TRUE), 100)
)

pred_probs4_nb <- predict(nb_model, newdata = pdp_data4_nb, type = "raw")

success_probability4 <- pred_probs4_nb[, 2]

pdp_data4_nb$success_probability4 <- success_probability4

ggplot(pdp_data4_nb, aes(x = release_year, y = success_probability4)) +
  geom_line() +
  labs(title = "Partial Dependence of Success on release year (nb)",
       x = "release year",
       y = "Probability of Success") +
  theme_minimal()
```

## Classifying Success with Shrinkage:

Shrinkage methods we will use are Lasso and Ridge Regression. Lasso helps with feature selection and Ridge Regression helps with handling multicollinearity.

### Ridge Regression

Ridge regression reduces variance in the presence of highly correlated predictors like budget and popularity, ensuring effective predictions. Although all predictors are kept, their coefficients are shrunk, reflecting their relative importance. For instance, popularity has a higher coefficient than the, indicating its stronger influence on predicting success. This is useful because some filmmakers may want a holistic view of all contributing factors, even those with smaller effects.

#### Training the Model

We will fit the Ridge model with cross-validation to find the optimal lambda. Then, we will use that best lambda to train the model.

```{r}
library(glmnet)

x <- model.matrix(success ~ budget + release_year + runtime + popularity, data = training_n)[, -1]  
y <- ifelse(training_n$success == "Success", 1, 0)  

set.seed(123) 
ridge_cv <- cv.glmnet(x, y, alpha = 0, family = "binomial") 

best_lambda_ridge <- ridge_cv$lambda.min
print(paste("Optimal Lambda for Ridge Regression: ", best_lambda_ridge))

ridge_model <- glmnet(x, y, alpha = 0, family = "binomial", lambda = best_lambda_ridge)

```

#### Predicting

```{r}

ridge_probabilities <- predict(ridge_model, newx = x, type = "response")

ridge_predictions <- ifelse(ridge_probabilities > 0.5, 1, 0)


ridge_prediction_counts <- table(ridge_predictions)
print(ridge_prediction_counts)

```

#### Evaluating the Model

We will use a confusion matrix to evaluate the model accuracy.

```{r}
ridge_confusion_matrix <- table(Predicted = ridge_predictions, Actual = y)
print(ridge_confusion_matrix)


ridge_accuracy <- sum(ridge_predictions == y) / length(y)
print(paste("Accuracy of Ridge Regression: ", round(ridge_accuracy * 100, 4), "%"))

ridge_coefficients <- as.matrix(coef(ridge_model))
print(ridge_coefficients)

```

We can see that popularity has a coefficient of about 53.72, while the other variables have coefficients closer to 0. This shows that popularity has the biggest (positive) influence on predicting success. \###

## Lasso Regression

#### Training the Model

We will fit the Lasso model with cross-validation to find the optimal lambda. Then, using this best lambda we will train the Lasso model with it.

```{r}

set.seed(123) 
lasso_cv <- cv.glmnet(x, y, alpha = 1, family = "binomial") 

best_lambda_lasso <- lasso_cv$lambda.min
print(paste("Optimal Lambda for Lasso Regression: ", best_lambda_lasso))


lasso_model <- glmnet(x, y, alpha = 1, family = "binomial", lambda = best_lambda_lasso)

```

#### Predicting

We will use a threshold of 0.5 to classify a success or not.

```{r}
lasso_probabilities <- predict(lasso_model, newx = x, type = "response")


lasso_predictions <- ifelse(lasso_probabilities > 0.5, 1, 0)

lasso_prediction_counts <- table(lasso_predictions)
print(lasso_prediction_counts)

```

#### Evaluating the Model

We will use a confusion matrix to evaluate the model accuracy.

```{r}
lasso_confusion_matrix <- table(Predicted = lasso_predictions, Actual = y)
print(lasso_confusion_matrix)

lasso_accuracy <- sum(lasso_predictions == y) / length(y)
print(paste("Accuracy of Lasso Regression: ", round(lasso_accuracy * 100, 4), "%"))

lasso_coefficients <- as.matrix(coef(lasso_model))
print(lasso_coefficients)

```

Lasso regression automatically sets some coefficients to zero, removing less important predictors like budget if they do not significantly contribute to the model. By focusing on a smaller set of predictors, Lasso provides a more interpretable model. Lasso confirms Ridge's output by also revealing that the coefficient for popularity is the largest, at about 87.51. This means that popularity has a strong positive relationship on predicting success. This helps filmmakers focus on the most influential factors, reducing unnecessary expenditures on less critical aspects to make a movie successful.

## Classifying Success with Logistic Regression:

Objective is to predict whether a movie is successful using logistic regression based on budget, release year, runtime, and popularity.

### Training the model

```{r}
testing_lg = testing_n
training_lg = training_n

testing_lg$success <- ifelse(testing_lg$success == "Success", 1, 0)
training_lg$success <- ifelse(training_lg$success == "Success", 1, 0)

lg_model <- glm(success ~ budget + release_year + runtime + popularity,
                data = training_lg, 
                family = "binomial")


```

### Predicting:

```{r}
probabilities_lg <- predict(lg_model, newdata = testing_lg, type = "response")

predictions_lg <- ifelse(probabilities_lg > 0.5, 1, 0)

prediction_lg_counts <- table(predictions_lg)
print(prediction_lg_counts)
```

### Evaluating the model

```{r}
confusion_matrix_lg <- table(Predicted = predictions_lg, Actual = testing_lg$success)
print(confusion_matrix_lg)

accuracy_lg <- sum(predictions_lg == testing_lg$success) / length(predictions_lg)
print(paste("Accuracy: ", round(accuracy_lg * 100, 4), "%"))

```

The confusion matrix shows the distribution of correct and incorrect predictions. The accuracy percentage provides a measure of how well the model predicts movie success.

### PDP

```{r}
pdp_budget_lg <- pdp::partial(lg_model, pred.var = "popularity")

ggplot(pdp_budget_lg, aes(x = popularity, y = yhat)) +
  geom_line() +
  ggtitle("Partial Dependence of Success on popularity (Logistic Regression)") +
  xlab("popularity") +
  ylab("Predicted Probability of Success")

pdp_budget_lg2 <- pdp::partial(lg_model, pred.var = "budget")

ggplot(pdp_budget_lg2, aes(x = budget, y = yhat)) + 
  geom_line() +
  ggtitle("Partial Dependence of Success on budget") +
  xlab("budget") +
  ylab("Predicted Probability of Success") 

pdp_budget_lg3 <- pdp::partial(lg_model, pred.var = "runtime")

ggplot(pdp_budget_lg3, aes(x = runtime, y = yhat)) + 
  geom_line() +
  ggtitle("Partial Dependence of Success on runtime") +
  xlab("runtime") +
  ylab("Predicted Probability of Success") 

pdp_budget_lg4 <- pdp::partial(lg_model, pred.var = "release_year")

ggplot(pdp_budget_lg4, aes(x = release_year, y = yhat)) + 
  geom_line() +
  ggtitle("Partial Dependence of Success on release_year") +
  xlab("release_year") +
  ylab("Predicted Probability of Success") 
```

## Classifying Budget into four predicted groups with multinomial logistic regression:

```{r}
if (!require("nnet")) install.packages("nnet")
library(nnet)
```

### Preparing target

```{r}
budget_quartiles <- quantile(training_n$budget, probs = c(0, 0.25, 0.5, 0.75, 1), na.rm = TRUE)
budget_quartiles <- unique(budget_quartiles)  

if (length(budget_quartiles) - 1 != 4) {
  stop("Unable to create exactly 4 quartile groups due to duplicate breaks. Please inspect the data.")
}

# Assign budget categories
training_n$budget_category <- cut(
  training_n$budget, 
  breaks = budget_quartiles, 
  labels = c("Low", "Medium", "High", "Very High"), 
  include.lowest = TRUE
)

table(training_n$budget_category)

```

### Fitting the model

```{r}
training_n$budget_category <- as.factor(training_n$budget_category)

multinom_model <- multinom(budget_category ~ revenue + release_year + popularity, data = training_n)
```

### Check the model summary

```{r}
summary(multinom_model)
```

### Predict classifications

```{r}
predicted_categories = predict(multinom_model, newdata = testing_n)

category_counts = table(predicted_categories)
print(category_counts)

```

### Evaluating model with confusion matrix and calculating accuracy

```{r}

table(Predicted = predicted_categories, Actual = testing_n$budget_category)

accuracy <- mean(predicted_categories == testing_n$budget_category)
cat("Accuracy: ", round(accuracy * 100, 4), "%")

```

# Classification with Emphasis on Interpretation

## Classifying Success with Decision Trees

The decision tree structure shows how features like budget and popularity split the data to classify movies. The path from root to leaf highlights the decision rules. This easily identifies the most important features based on the splits.

### Fit the Model

```{r}
tree_model <- rpart(success ~ budget + release_year + runtime + popularity, data = training, method = "class")
```

### Visualize the Trees

```{r}
if (!require("rpart.plot")) install.packages("rpart.plot")
library(rpart.plot)
rpart.plot(tree_model, type = 3, extra = 101, under = TRUE, fallen.leaves = TRUE)
```

### Predicting

```{r}
tree_predictions <- predict(tree_model, newdata = testing, type = "class")
```

### Evaluating the Model

Use the confusion matrix to evaluate the model.

```{r}
tree_confusion <- table(Predicted = tree_predictions, Actual = testing$success)
accuracy_tree <- sum(tree_predictions == testing$success) / nrow(testing)

cat("Decision Tree Confusion Matrix:\n")
print(tree_confusion)
cat("Decision Tree Accuracy: ", round(accuracy_tree * 100, 4), "%\n") 
```

### PDP

```{r}
pdp_budget_tree <- pdp::partial(tree_model, pred.var = "release_year")

ggplot(pdp_budget_tree, aes(x = release_year, y = yhat)) + 
  geom_line() +
  ggtitle("Partial Dependence of Success on release_year") +
  xlab("release_year") +
  ylab("Predicted Probability of Success") 

pdp_budget_tree2 <- pdp::partial(tree_model, pred.var = "budget")

ggplot(pdp_budget_tree2, aes(x = budget, y = yhat)) + 
  geom_line() +
  ggtitle("Partial Dependence of Success on budget") +
  xlab("budget") +
  ylab("Predicted Probability of Success") 

pdp_budget_tree3 <- pdp::partial(tree_model, pred.var = "popularity")

ggplot(pdp_budget_tree3, aes(x = popularity, y = yhat)) + 
  geom_line() +
  ggtitle("Partial Dependence of Success on popularity") +
  xlab("popularity") +
  ylab("Predicted Probability of Success") 

pdp_budget_tree4 <- pdp::partial(tree_model, pred.var = "runtime")

ggplot(pdp_budget_tree4, aes(x = runtime, y = yhat)) + 
  geom_line() +
  ggtitle("Partial Dependence of Success on runtime") +
  xlab("runtime") +
  ylab("Predicted Probability of Success") 


```

## Classifying Success with Random Forest (with Feature Importance)

### Fit the Model

```{r}
library(randomForest)

rf_model <- randomForest(success ~ budget + release_year + runtime + popularity, data = training, importance = TRUE)

```

### View Variable Importance

This shows which features contribute most to the model.

```{r}
importance <- importance(rf_model)
varImpPlot(rf_model)
```

We can see popularity contributes the most to the model with budget next, then release year, and runtime being the least contributable predictor. \### Prediction

```{r}
rf_predictions <- predict(rf_model, newdata = testing)
```

### Evaluating the Model

Use a confusion matrix to evaluate the model.

```{r}
rf_confusion <- table(Predicted = rf_predictions, Actual = testing$success)
accuracy_rf <- sum(rf_predictions == testing$success) / nrow(testing)

cat("Random Forest Confusion Matrix:\n")
print(rf_confusion)
cat("Random Forest Accuracy: ", round(accuracy_rf * 100, 4), "%\n")

```

### PDP

```{r}
pdp_budget_rf <- pdp::partial(rf_model, pred.var = "release_year")

ggplot(pdp_budget_rf, aes(x = release_year, y = yhat)) + 
  geom_line() +
  ggtitle("Partial Dependence of Success on release_year") +
  xlab("release_year") +
  ylab("Predicted Probability of Success") 

pdp_budget_rf2 <- pdp::partial(rf_model, pred.var = "runtime")

ggplot(pdp_budget_rf2, aes(x = runtime, y = yhat)) + 
  geom_line() +
  ggtitle("Partial Dependence of Success on runtime") +
  xlab("runtime") +
  ylab("Predicted Probability of Success") 

pdp_budget_rf3 <- pdp::partial(rf_model, pred.var = "popularity")

ggplot(pdp_budget_rf3, aes(x = popularity, y = yhat)) + 
  geom_line() +
  ggtitle("Partial Dependence of Success on popularity") +
  xlab("popularity") +
  ylab("Predicted Probability of Success") 

pdp_budget_rf4 <- pdp::partial(rf_model, pred.var = "budget")

ggplot(pdp_budget_rf4, aes(x = budget, y = yhat)) + 
  geom_line() +
  ggtitle("Partial Dependence of Success on budget") +
  xlab("budget") +
  ylab("Predicted Probability of Success") 


```

## Classifying Success with Neural Networks

In the context of movie success classification, the neural network captures nonlinear relationships between features like budget and popularity interacting in unexpected ways.

### Fit the Model

```{r}
set.seed(123)
df_normal$success <- as.factor(df_normal$success) 
trainIndex <- createDataPartition(df_normal$success, p = 0.8, list = FALSE)
trainData <- df_normal[trainIndex, ]
testData <- df_normal[-trainIndex, ]

nn_model <- nnet(success ~ budget + release_year + runtime + popularity,
                 data = trainData,
                 size = 5, 
                 decay = 0.01,  
                 maxit = 500)  

print(summary(nn_model))

```

### Prediction

```{r}
nn_predictions <- predict(nn_model, newdata = testData, type = "class")
```

### Evaluating the Model

Use the confusion matrix to evaluate model accuracy.

```{r}
confusion_matrix_nn <- table(Predicted = nn_predictions, Actual = testData$success)
print(confusion_matrix_nn)

accuracy_nn <- sum(diag(confusion_matrix_nn)) / sum(confusion_matrix_nn)
print(paste("Accuracy: ", round(accuracy_nn * 100, 4), "%"))

```

## Classifying with Gradient Boosting

Gradient boosting combines multiple weak learners (decision trees) to create a strong predictive model.

### Fit the Model

We will use a bernoulli distribution since we are working with binary classification.

```{r}
training$success <- ifelse(training$success == "Success", 1, 0)

set.seed(123)
gbm_model <- gbm(success ~ budget + release_year + runtime + popularity,
                 data = training,
                 distribution = "bernoulli", 
                 n.trees = 500,  
                 interaction.depth = 3,  
                 shrinkage = 0.01,  
                 cv.folds = 5)  
```

### Prediction

We will find the best number of decision trees to use for the probabilities using cross validation.

```{r}
best_iter <- gbm.perf(gbm_model, method = "cv")

gbm_probabilities <- predict(gbm_model, newdata = testing, n.trees = best_iter, type = "response")
gbm_predictions <- ifelse(gbm_probabilities > 0.5, "Success", "No Success")
```

### Evaluating the Model

Use the confusion matrix to evaluate the model accuracy.

```{r}
confusion_matrix_gbm <- table(Predicted = gbm_predictions, Actual = testing$success)
print(confusion_matrix_gbm)

accuracy_gbm <- sum(diag(confusion_matrix_gbm)) / sum(confusion_matrix_gbm)
print(paste("Accuracy: ", round(accuracy_gbm * 100, 4), "%"))

```

### Feature Importance

```{r}
importance <- summary(gbm_model)
print(importance)
```

We can see that this model predicts popularity to have the most relative influence, followed by release year, then budget, than runtime. This is different from the other models as the others predict budget as the second most influential variable. **Prediction** using Logistic regression answers the question, "Can we predict success?"

**Interpretation** using LDA highlights "Why are some movies predicted as successful or unsuccessful?" by examining variable relationships and decision boundaries.

# Feature Selection and Comparison of Predictor Sets

## Feature Importance Analysis

### Correlation Analysis for Continuous Variables

We will analyze the correlation between predictors and the target variable, success. We will identify multicollinearity among predictors to avoid redudancy. We will do so by looking at a correlation matrix for numeric predictors.

```{r}
numeric_vars <- df_for_class[, sapply(df_for_class, is.numeric)]
correlation_matrix <- cor(numeric_vars)
library(corrplot)
corrplot(correlation_matrix, method = "circle")

```

```{r}
selected_vars <- df_for_class[, c("budget", "release_year", "runtime", "popularity")]

correlation_matrix <- cor(selected_vars)

library(corrplot)
corrplot(correlation_matrix, method = "circle")

```

Checking to make sure there is no multicollinearity present for the variables we chose to predict. We can see that there is no multicollinearity present as the correlations between the variables we chose are small.

### Variable Importance from Random Forest

Random Forest provides a direct measure of feature importance.

```{r}
rf_model <- randomForest(success ~ budget + release_year + runtime + popularity, data = df_for_class, importance = TRUE)
varImpPlot(rf_model)
```

We already saw above when we did the random forest model that random forest predicts popularity to be the most influential predictor on success. However when we do random forest on the data set and not the training the second most influential variable changes. For MeanDecreaseAccuracy (which shows how a feature effects the overall prediction accuracy), it shows release_year next, followed by budget then runtime. For MeanDecreaseGini (which shows how a feature influences the splitting criteria) shows budget next, followed by release year and then runtime. \### Gradient Boosting Feature Importance and Partial Dependence

#### Feature Importance and Plot

```{r}
importance_df_gbm <- data.frame(
  Feature = summary(gbm_model)$var,
  Importance = summary(gbm_model)$rel.inf
)

ggplot(importance_df_gbm, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "darkorange") +
  coord_flip() +
  ggtitle("Feature Importances from Gradient Boosting") +
  xlab("") +
  ylab("Relative Influence") +
  theme_minimal()
```

Shows popularity, then release year, then budget, then runtime is the order of relative influence of predictor variables to the overall success.

### Stepwise Selection

We can use stepwise regression to identify the most relevant features for logistic regression.

```{r}
full_model <- glm(success ~ budget + release_year + runtime + popularity, data = df_for_class, family = binomial)
step_model <- stepAIC(full_model, direction = "both")
summary(step_model)
```

Release_year and popularity have strong effects on predicting the success of a movie. As release_year increases, success decreases, while higher popularity increases success. Runtime is somewhat important, but its effect is weaker compared to the other variables. Initially, the model included budget, release_year, runtime, and popularity. After stepwise selection, the final model excluded budget and only included release_year, runtime, and popularity. You can see this from the change in AIC values during the stepwise process. The AIC value decreased from 1313.79 to 1311.8 after the removal of budget, indicating a slightly better model fit when excluding this variable.

### SHAP Values for Model Interpretability

#### SHAP Analysis with Random Forest

```{r}
predict_function <- function(object, newdata) {
  predict(object, newdata = newdata, type = "prob")[, "Success"]
}

X <- horror[, c("budget", "release_year", "runtime", "popularity")]
y <- horror$success

```

#### Compute SHAP Values + Mean Absolute SHAP Values

```{r}

predict_function <- function(object, newdata) {
  predict(object, newdata = newdata, type = "prob")[, "Success"]
}

set.seed(123)
shap_values <- fastshap::explain(
  object = rf_model,
  X = df_for_class[, c("budget", "release_year", "runtime", "popularity")],  
  pred_wrapper = predict_function,
  nsim = 50,
  adjust = TRUE
)

mean_abs_shap <- colMeans(abs(shap_values))
shap_importance <- data.frame(
  Feature = names(mean_abs_shap),
  MeanAbsShap = mean_abs_shap
)
```

#### SHAP Feature Importance Plot

```{r}
ggplot(shap_importance, aes(x = reorder(Feature, MeanAbsShap), y = MeanAbsShap)) +
  geom_bar(stat = "identity", fill = "purple") +
  coord_flip() +
  ggtitle("SHAP Feature Importance for Random Forest") +
  xlab("") +
  ylab("Mean |SHAP Value|") +
  theme_minimal()

```

Again, this predicts the same as the other models with order of importance being popualrity, release_year, budget, runtime.

#### SHAP Dependence Plot 

Now we will plot a SHAP dependence plot for each variable to see the complete view of the model's decision-making process.

```{r}
shap_values_pop <- shap_values[, "popularity"]

ggplot(data = data.frame(
  SHAP_value = shap_values_pop,
  Feature_value = df_for_class$popularity  
), aes(x = Feature_value, y = SHAP_value)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "loess", se = FALSE, color = "blue") +
  ggtitle("SHAP Dependence Plot for Popularity") +
  xlab("Popularity") +
  ylab("SHAP Value") +
  theme_minimal()

shap_values_yr <- shap_values[, "release_year"]

ggplot(data = data.frame(
  SHAP_value = shap_values_yr,
  Feature_value = df_for_class$release_year  
), aes(x = Feature_value, y = SHAP_value)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "loess", se = FALSE, color = "blue") +
  ggtitle("SHAP Dependence Plot for Release Year") +
  xlab("Release Year") +
  ylab("SHAP Value") +
  theme_minimal()

shap_values_budget <- shap_values[, "budget"]

ggplot(data = data.frame(
  SHAP_value = shap_values_budget,
  Feature_value = df_for_class$budget  # Original feature values for 'budget'
), aes(x = Feature_value, y = SHAP_value)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "loess", se = FALSE, color = "blue") +
  ggtitle("SHAP Dependence Plot for Budget") +
  xlab("Budget") +
  ylab("SHAP Value") +
  theme_minimal()

shap_values_runtime <- shap_values[, "runtime"]

ggplot(data = data.frame(
  SHAP_value = shap_values_runtime,
  Feature_value = df_for_class$runtime  
), aes(x = Feature_value, y = SHAP_value)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "loess", se = FALSE, color = "blue") +
  ggtitle("SHAP Dependence Plot for Runtime") +
  xlab("Runtime") +
  ylab("SHAP Value") +
  theme_minimal()


```

#### RFE for Feature Selection

```{r}
selected_features_df <- numeric_vars[, c("budget", "release_year", "runtime", "popularity")]

ctrl <- rfeControl(functions = rfFuncs, method = "cv", number = 5)

rfe_result <- rfe(selected_features_df, numeric_vars$success, sizes = c(1:ncol(selected_features_df)), rfeControl = ctrl)

print(rfe_result)

selected_features <- rfe_result$optVariables
print(selected_features)

```

### Multiple Regression with t-values for Variable Importance

```{r}
summary(lg_model)

```

We can use Pr\|z\| to find statistically significant features. Since release year and popularity are the variables with p values less than 0.05, we can say these are the variables that are statistically significant at predicting success.

### Feature Selection with all variables in the dataset 

#### Z-scores for Most Significant Variables

```{r}
numeric_vars <- df_for_class[, sapply(df_for_class, is.numeric)]
numeric_vars$success <- df_for_class$success
lg_model2 <- glm(success ~ ., data = numeric_vars, family = binomial)
summary(lg_model2)
```

We can use Pr\|z\| to find statistically significant features. When looking at the model with every variable, the only two with p values less than 0.05 and therefore are statistically significant at predicting success are revenue and budget. However, we need to be cautious because these are correlated.

```{r}
step_model <- step(lg_model, direction = "both", trace = 0)

summary(step_model)

```

After using the STEP model, it shows that release_year and popularity are the 2 features that were selected.

```{r}

ctrl <- rfeControl(functions = rfFuncs, method = "cv", number = 10)

rfe_result <- rfe(numeric_vars[, -which(names(numeric_vars) == "success")], numeric_vars$success, sizes = c(1:ncol(numeric_vars)-1), rfeControl = ctrl)

print(rfe_result)

selected_features <- rfe_result$optVariables
print(selected_features)


```

This selects profit, however this is directly correlated with success so we need to be weary.

## Comparing Predictor Sets

### Base Set of Predictors

We will use all of our predictors: budget, release_year, runtime, and popularity to build our base model using logistic regression. 

```{r}
base_model = lg_model
base_accuracy <- accuracy_lg
cat("Base Model Accuracy: ", base_accuracy, "\n")


```

### Reduced Set of Predictors

We will now use only the most important predictors identifies through feature selection which are popularity and release year. 

```{r}
reduced_model <- glm(success ~ release_year + popularity, data = training_lg, family = binomial)
reduced_probs <- predict(reduced_model, newdata = testing_lg, type = "response")
reduced_preds <- ifelse(reduced_probs > 0.5, 1, 0)
reduced_accuracy <- sum(reduced_preds == testing_lg$success) / length(reduced_preds)
cat("Reduced Model Accuracy: ", reduced_accuracy, "\n")

```
We can see that the reduced model performs slightly better on new data. 

### Comparing Models with Different Predictor Sets
We have one example of a reduced model above. Let's continue and keep comparing different predictor sets all including popularity since this was unanimously the most influential feature predicting success. We will train and evaluate models with different predictor sets for Random Forest and train and evaluate models with different predictor sets for Multiple Linear Regression.

```{r}
predictor_set_1 <- c("budget", "popularity")
predictor_set_2 <- c("release_year", "popularity")
predictor_set_3 <- c("budget", "release_year", "popularity")
predictor_set_4 <- c("budget", "release_year", "runtime", "popularity")

train_rf_model <- function(predictors, data) {
  model <- randomForest(success ~ ., data = data[, c(predictors, "success")], ntree = 500)
  prob <- predict(model, type = "prob")[,2]

  pred <- prediction(prob, data$success)
  perf <- performance(pred, measure = "auc")
  auc <- perf@y.values[[1]]
  
  return(auc)
}

train_lm_model <- function(predictors, data) {
  model <- lm(success ~ ., data = data[, c(predictors, "success")])
  
  prob <- predict(model, type = "response")
  pred_labels <- ifelse(prob > 0.5, 1, 0)
  
  accuracy <- mean(pred_labels == data$success)
  
  return(accuracy)
}

rf_auc_1 <- train_rf_model(predictor_set_1, df_for_class)
rf_auc_2 <- train_rf_model(predictor_set_2, df_for_class)
rf_auc_3 <- train_rf_model(predictor_set_3, df_for_class)
rf_auc_4 <- train_rf_model(predictor_set_4, df_for_class)

df_for_class_lg = df_for_class
df_for_class_lg$success <- as.numeric(as.factor(df_for_class$success)) - 1

lm_accuracy_1 <- train_lm_model(predictor_set_1, df_for_class_lg)
lm_accuracy_2 <- train_lm_model(predictor_set_2, df_for_class_lg)
lm_accuracy_3 <- train_lm_model(predictor_set_3, df_for_class_lg)
lm_accuracy_4 <- train_lm_model(predictor_set_4, df_for_class_lg)

performance_comparison <- data.frame(
  Model = c("Random Forest (Set 1)", "Random Forest (Set 2)", "Random Forest (Set 3)", "Random Forest (Set 4)",
            "Linear Regression (Set 1)", "Linear Regression (Set 2)", "Linear Regression (Set 3)", "Linear Regression (Set 4)"),
  AUC_or_Accuracy = c(rf_auc_1, rf_auc_2, rf_auc_3, rf_auc_4,
                      lm_accuracy_1, lm_accuracy_2, lm_accuracy_3, lm_accuracy_4)
)

print(performance_comparison)


```

### Plotting Performance Comparison

```{r}
# Plot the comparison
library(ggplot2)

ggplot(performance_comparison, aes(x = Model, y = AUC_or_Accuracy, fill = Model)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  coord_flip() +
  theme_minimal() +
  ggtitle("Comparison of Model Performance across Different Predictor Sets") +
  xlab("Model and Predictor Set") +
  ylab("AUC (RF) / Accuracy (LM)")

```

### Adding Interaction Terms

We can test whether interaction terms improve model performance.

```{r}
interaction_model <- glm(success ~ budget * popularity + release_year, data = df_for_class_lg, family = binomial)
interaction_probs <- predict(interaction_model, type = "response")
interaction_preds <- ifelse(interaction_probs > 0.5, 1, 0)
interaction_accuracy <- mean(interaction_preds == df_for_class_lg$success)
cat("Interaction Model Accuracy: ", interaction_accuracy, "\n")

```

### Cross-Validation to Compare Models

Use cross-validation to evaluate the generalizability of each predictor set.

```{r}
# Base Model
base_cv <- train(success ~ budget + release_year + runtime + popularity, data = df_for_class, method = "glm", family = binomial, trControl = trainControl(method = "cv", number = 5))
print(base_cv)

# Reduced Model
reduced_cv <- train(success ~ budget + popularity + release_year, data = df_for_class, method = "glm", family = binomial, trControl = trainControl(method = "cv", number = 5))
print(reduced_cv)
```

## Visualize Feature Importance

### Effect Plots for Logistic Regression

Visualize the effect of individual predictors on the probability of success.

```{r}
model3 <- glm(success ~ budget + release_year + popularity, data = df_for_class_lg, family = binomial)
effect_plot <- allEffects(model3)
plot(effect_plot)
```


